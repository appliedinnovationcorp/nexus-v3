version: '3.8'

services:
  # Apache Kafka for Event Streaming
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - data-pipeline

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - kafka_data:/var/lib/kafka/data
    depends_on:
      - zookeeper
    networks:
      - data-pipeline
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kafka UI for Management
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      - kafka
    networks:
      - data-pipeline

  # ClickHouse for Data Warehouse
  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    container_name: clickhouse
    ports:
      - "8123:8123"  # HTTP interface
      - "9000:9000"  # Native interface
    environment:
      CLICKHOUSE_DB: analytics
      CLICKHOUSE_USER: analytics_user
      CLICKHOUSE_PASSWORD: analytics_password
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./config/clickhouse/config.xml:/etc/clickhouse-server/config.xml
      - ./config/clickhouse/users.xml:/etc/clickhouse-server/users.xml
      - ./sql/clickhouse-init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - data-pipeline
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 5s
      retries: 3

  # Apache Airflow for ETL Orchestration
  postgres-airflow:
    image: postgres:15-alpine
    container_name: postgres-airflow
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data
    networks:
      - data-pipeline

  redis-airflow:
    image: redis:7-alpine
    container_name: redis-airflow
    volumes:
      - redis_airflow_data:/data
    networks:
      - data-pipeline

  airflow-webserver:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    ports:
      - "8081:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis-airflow:6379/0
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./config/airflow:/opt/airflow/config
    depends_on:
      - postgres-airflow
      - redis-airflow
    networks:
      - data-pipeline
    command: webserver

  airflow-scheduler:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis-airflow:6379/0
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./config/airflow:/opt/airflow/config
    depends_on:
      - postgres-airflow
      - redis-airflow
    networks:
      - data-pipeline
    command: scheduler

  airflow-worker:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    container_name: airflow-worker
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis-airflow:6379/0
      AIRFLOW__CORE__FERNET_KEY: ''
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./config/airflow:/opt/airflow/config
    depends_on:
      - postgres-airflow
      - redis-airflow
    networks:
      - data-pipeline
    command: celery worker

  # Apache Spark for Big Data Processing
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    ports:
      - "8082:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master Port
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - spark_master_data:/opt/bitnami/spark/data
      - ./spark/jobs:/opt/spark-jobs
      - ./spark/jars:/opt/spark-jars
    networks:
      - data-pipeline

  spark-worker-1:
    image: bitnami/spark:3.5
    container_name: spark-worker-1
    ports:
      - "8083:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - spark_worker1_data:/opt/bitnami/spark/data
      - ./spark/jobs:/opt/spark-jobs
      - ./spark/jars:/opt/spark-jars
    depends_on:
      - spark-master
    networks:
      - data-pipeline

  spark-worker-2:
    image: bitnami/spark:3.5
    container_name: spark-worker-2
    ports:
      - "8084:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - spark_worker2_data:/opt/bitnami/spark/data
      - ./spark/jobs:/opt/spark-jobs
      - ./spark/jars:/opt/spark-jars
    depends_on:
      - spark-master
    networks:
      - data-pipeline

  # Apache Superset for Business Intelligence
  superset-postgres:
    image: postgres:15-alpine
    container_name: superset-postgres
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
    volumes:
      - superset_postgres_data:/var/lib/postgresql/data
    networks:
      - data-pipeline

  superset-redis:
    image: redis:7-alpine
    container_name: superset-redis
    volumes:
      - superset_redis_data:/data
    networks:
      - data-pipeline

  superset:
    build:
      context: ./docker/superset
      dockerfile: Dockerfile
    container_name: superset
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_CONFIG_PATH=/app/superset_config.py
      - SUPERSET_SECRET_KEY=your-secret-key-here
    volumes:
      - ./config/superset/superset_config.py:/app/superset_config.py
      - superset_home:/app/superset_home
    depends_on:
      - superset-postgres
      - superset-redis
      - clickhouse
    networks:
      - data-pipeline

  # Event Tracking API
  event-tracker:
    build:
      context: ./docker/event-tracker
      dockerfile: Dockerfile
    container_name: event-tracker
    ports:
      - "3500:3500"
    environment:
      - NODE_ENV=production
      - KAFKA_BROKERS=kafka:29092
      - CLICKHOUSE_URL=http://clickhouse:8123
      - CLICKHOUSE_DATABASE=analytics
      - CLICKHOUSE_USER=analytics_user
      - CLICKHOUSE_PASSWORD=analytics_password
      - REDIS_URL=redis://event-tracker-redis:6379
    volumes:
      - ./config/event-tracker:/app/config
      - ./logs:/app/logs
    depends_on:
      - kafka
      - clickhouse
      - event-tracker-redis
    networks:
      - data-pipeline

  event-tracker-redis:
    image: redis:7-alpine
    container_name: event-tracker-redis
    volumes:
      - event_tracker_redis_data:/data
    networks:
      - data-pipeline

  # Real-time Analytics Engine
  analytics-engine:
    build:
      context: ./docker/analytics-engine
      dockerfile: Dockerfile
    container_name: analytics-engine
    ports:
      - "3501:3501"
    environment:
      - NODE_ENV=production
      - KAFKA_BROKERS=kafka:29092
      - CLICKHOUSE_URL=http://clickhouse:8123
      - CLICKHOUSE_DATABASE=analytics
      - CLICKHOUSE_USER=analytics_user
      - CLICKHOUSE_PASSWORD=analytics_password
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./config/analytics-engine:/app/config
      - ./logs:/app/logs
    depends_on:
      - kafka
      - clickhouse
      - spark-master
    networks:
      - data-pipeline

  # A/B Testing Framework
  ab-testing-service:
    build:
      context: ./docker/ab-testing-service
      dockerfile: Dockerfile
    container_name: ab-testing-service
    ports:
      - "3502:3502"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://ab_user:ab_password@ab-testing-postgres:5432/ab_testing
      - REDIS_URL=redis://ab-testing-redis:6379
      - KAFKA_BROKERS=kafka:29092
      - CLICKHOUSE_URL=http://clickhouse:8123
    volumes:
      - ./config/ab-testing:/app/config
      - ./logs:/app/logs
    depends_on:
      - ab-testing-postgres
      - ab-testing-redis
      - kafka
      - clickhouse
    networks:
      - data-pipeline

  ab-testing-postgres:
    image: postgres:15-alpine
    container_name: ab-testing-postgres
    environment:
      POSTGRES_USER: ab_user
      POSTGRES_PASSWORD: ab_password
      POSTGRES_DB: ab_testing
    volumes:
      - ab_testing_postgres_data:/var/lib/postgresql/data
      - ./sql/ab-testing-init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - data-pipeline

  ab-testing-redis:
    image: redis:7-alpine
    container_name: ab-testing-redis
    volumes:
      - ab_testing_redis_data:/data
    networks:
      - data-pipeline

  # Customer Journey Analytics
  journey-analytics:
    build:
      context: ./docker/journey-analytics
      dockerfile: Dockerfile
    container_name: journey-analytics
    ports:
      - "3503:3503"
    environment:
      - NODE_ENV=production
      - CLICKHOUSE_URL=http://clickhouse:8123
      - CLICKHOUSE_DATABASE=analytics
      - CLICKHOUSE_USER=analytics_user
      - CLICKHOUSE_PASSWORD=analytics_password
      - KAFKA_BROKERS=kafka:29092
    volumes:
      - ./config/journey-analytics:/app/config
      - ./logs:/app/logs
    depends_on:
      - clickhouse
      - kafka
    networks:
      - data-pipeline

  # Data Quality Monitor
  data-quality-monitor:
    build:
      context: ./docker/data-quality-monitor
      dockerfile: Dockerfile
    container_name: data-quality-monitor
    ports:
      - "3504:3504"
    environment:
      - NODE_ENV=production
      - CLICKHOUSE_URL=http://clickhouse:8123
      - CLICKHOUSE_DATABASE=analytics
      - PROMETHEUS_URL=http://data-pipeline-prometheus:9090
    volumes:
      - ./config/data-quality:/app/config
      - ./logs:/app/logs
    depends_on:
      - clickhouse
      - data-pipeline-prometheus
    networks:
      - data-pipeline

  # Prometheus for Data Pipeline Metrics
  data-pipeline-prometheus:
    image: prom/prometheus:latest
    container_name: data-pipeline-prometheus
    ports:
      - "9098:9090"
    volumes:
      - ./config/prometheus/data-pipeline-prometheus.yml:/etc/prometheus/prometheus.yml
      - data_pipeline_prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - data-pipeline

  # Grafana for Data Pipeline Monitoring
  data-pipeline-grafana:
    image: grafana/grafana:latest
    container_name: data-pipeline-grafana
    ports:
      - "3310:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel,grafana-clock-panel
    volumes:
      - data_pipeline_grafana_data:/var/lib/grafana
      - ./config/grafana/data-pipeline-provisioning:/etc/grafana/provisioning
      - ./config/grafana/data-pipeline-dashboards:/var/lib/grafana/dashboards
    networks:
      - data-pipeline

  # Schema Registry for Kafka
  schema-registry:
    image: confluentinc/cp-schema-registry:7.4.0
    container_name: schema-registry
    ports:
      - "8085:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    depends_on:
      - kafka
    networks:
      - data-pipeline

  # Kafka Connect for Data Integration
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.4.0
    container_name: kafka-connect
    ports:
      - "8086:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
    depends_on:
      - kafka
      - schema-registry
    networks:
      - data-pipeline

volumes:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  clickhouse_data:
  postgres_airflow_data:
  redis_airflow_data:
  spark_master_data:
  spark_worker1_data:
  spark_worker2_data:
  superset_postgres_data:
  superset_redis_data:
  superset_home:
  event_tracker_redis_data:
  ab_testing_postgres_data:
  ab_testing_redis_data:
  data_pipeline_prometheus_data:
  data_pipeline_grafana_data:

networks:
  data-pipeline:
    driver: bridge
