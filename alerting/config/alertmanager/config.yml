global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@nexus-v3.local'
  smtp_auth_username: 'alerts@nexus-v3.local'
  smtp_auth_password: 'password'
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
  opsgenie_api_url: 'https://api.opsgenie.com/'
  victorops_api_url: 'https://alert.victorops.com/integrations/generic/20131114/alert/'

# Template definitions
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route configuration with smart escalation
route:
  group_by: ['alertname', 'cluster', 'service', 'severity']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'default-receiver'
  
  routes:
    # Critical alerts - immediate escalation
    - match:
        severity: critical
      receiver: 'critical-escalation'
      group_wait: 5s
      group_interval: 2m
      repeat_interval: 5m
      routes:
        # P0 incidents - immediate page
        - match:
            priority: P0
          receiver: 'p0-immediate'
          group_wait: 0s
          repeat_interval: 2m
        
        # Security incidents
        - match:
            category: security
          receiver: 'security-team'
          group_wait: 0s
          repeat_interval: 1m
    
    # High severity alerts - escalated response
    - match:
        severity: high
      receiver: 'high-severity-escalation'
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 30m
    
    # SLO violations - SRE team
    - match:
        type: slo_violation
      receiver: 'sre-team'
      group_wait: 1m
      group_interval: 10m
      repeat_interval: 1h
    
    # Performance budget violations
    - match:
        type: performance_budget
      receiver: 'performance-team'
      group_wait: 2m
      group_interval: 15m
      repeat_interval: 2h
    
    # Infrastructure alerts
    - match:
        category: infrastructure
      receiver: 'infrastructure-team'
      group_interval: 10m
      repeat_interval: 1h
    
    # Application alerts
    - match:
        category: application
      receiver: 'application-team'
      group_interval: 5m
      repeat_interval: 30m
    
    # Chaos engineering alerts
    - match:
        source: chaos_engineering
      receiver: 'chaos-team'
      group_interval: 30m
      repeat_interval: 4h
    
    # Warning alerts - lower priority
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_interval: 15m
      repeat_interval: 4h

# Receiver definitions with escalation policies
receivers:
  # Default receiver
  - name: 'default-receiver'
    webhook_configs:
      - url: 'http://grafana-oncall-engine:8080/integrations/v1/alertmanager/default/'
        send_resolved: true

  # Critical escalation chain
  - name: 'critical-escalation'
    # Immediate notification
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts-critical'
        title: 'üö® CRITICAL ALERT'
        text: |
          *Alert:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Severity:* {{ .CommonLabels.severity }}
          *Service:* {{ .CommonLabels.service }}
          *Environment:* {{ .CommonLabels.environment }}
          
          {{ range .Alerts }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
        send_resolved: true
        actions:
          - type: button
            text: 'Acknowledge'
            url: 'http://localhost:8081/alerts/{{ .GroupKey }}/ack'
          - type: button
            text: 'Silence'
            url: 'http://localhost:9093/#/silences/new'
    
    # PagerDuty integration
    pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        severity: 'critical'
        details:
          alert_count: '{{ len .Alerts }}'
          service: '{{ .CommonLabels.service }}'
          environment: '{{ .CommonLabels.environment }}'
        links:
          - href: 'http://localhost:3000/d/alerts'
            text: 'Grafana Dashboard'
          - href: 'http://localhost:5601'
            text: 'Kibana Logs'
    
    # Email escalation
    email_configs:
      - to: 'oncall@nexus-v3.local'
        subject: '[CRITICAL] {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        body: |
          Critical Alert Triggered
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          Severity: {{ .Labels.severity }}
          Started: {{ .StartsAt }}
          
          Runbook: {{ .Annotations.runbook_url }}
          Dashboard: http://localhost:3000/d/{{ .Labels.service }}
          Logs: http://localhost:5601/app/logs
          {{ end }}
    
    # Webhook to incident management
    webhook_configs:
      - url: 'http://grafana-oncall-engine:8080/integrations/v1/alertmanager/critical/'
        send_resolved: true
        http_config:
          basic_auth:
            username: 'alertmanager'
            password: 'webhook_password'

  # P0 immediate response
  - name: 'p0-immediate'
    # Multiple channels for P0
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#incident-response'
        title: 'üî• P0 INCIDENT'
        text: |
          @channel IMMEDIATE ATTENTION REQUIRED
          
          *P0 Incident:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Impact:* Service completely down or major data loss
          *Service:* {{ .CommonLabels.service }}
          
          *Incident Commander:* Please respond immediately
          *War Room:* #incident-{{ .GroupKey }}
        send_resolved: true
    
    # SMS/Phone notifications (via webhook to external service)
    webhook_configs:
      - url: 'http://notification-service:8080/sms/emergency'
        send_resolved: true
        http_config:
          basic_auth:
            username: 'emergency'
            password: 'sms_password'

  # Security team alerts
  - name: 'security-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#security-alerts'
        title: 'üõ°Ô∏è SECURITY ALERT'
        text: |
          *Security Alert:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Type:* {{ .CommonLabels.alert_type }}
          *Source:* {{ .CommonLabels.source_ip }}
          *Target:* {{ .CommonLabels.target }}
          
          {{ range .Alerts }}
          *Details:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Time:* {{ .StartsAt }}
          {{ end }}
        send_resolved: true
    
    email_configs:
      - to: 'security@nexus-v3.local'
        subject: '[SECURITY] {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        body: |
          Security Alert Details:
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Source IP: {{ .Labels.source_ip }}
          Target: {{ .Labels.target }}
          Time: {{ .StartsAt }}
          
          Investigation required immediately.
          {{ end }}

  # SRE team for SLO violations
  - name: 'sre-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#sre-alerts'
        title: 'üìä SLO VIOLATION'
        text: |
          *SLO Violation:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Service:* {{ .CommonLabels.service }}
          *SLO:* {{ .CommonLabels.slo_name }}
          *Current SLI:* {{ .CommonLabels.sli_value }}
          *Target:* {{ .CommonLabels.slo_target }}
          *Error Budget:* {{ .CommonLabels.error_budget_remaining }}%
          
          {{ range .Alerts }}
          *Impact:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
        send_resolved: true
    
    webhook_configs:
      - url: 'http://grafana-oncall-engine:8080/integrations/v1/alertmanager/slo/'
        send_resolved: true

  # Performance team for budget violations
  - name: 'performance-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#performance-alerts'
        title: '‚ö° PERFORMANCE BUDGET VIOLATION'
        text: |
          *Performance Alert:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Service:* {{ .CommonLabels.service }}
          *Metric:* {{ .CommonLabels.metric_name }}
          *Current Value:* {{ .CommonLabels.current_value }}
          *Budget:* {{ .CommonLabels.budget_value }}
          *Regression:* {{ .CommonLabels.regression_percentage }}%
          
          {{ range .Alerts }}
          *Details:* {{ .Annotations.description }}
          *Dashboard:* {{ .Annotations.dashboard_url }}
          {{ end }}
        send_resolved: true

  # Infrastructure team
  - name: 'infrastructure-team'
    email_configs:
      - to: 'infrastructure@nexus-v3.local'
        subject: '[INFRA] {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        body: |
          Infrastructure Alert:
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Severity: {{ .Labels.severity }}
          Time: {{ .StartsAt }}
          {{ end }}
    
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#infrastructure'
        title: 'üèóÔ∏è Infrastructure Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        send_resolved: true

  # Application team
  - name: 'application-team'
    email_configs:
      - to: 'developers@nexus-v3.local'
        subject: '[APP] {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        body: |
          Application Alert:
          
          {{ range .Alerts }}
          Service: {{ .Labels.service }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Time: {{ .StartsAt }}
          {{ end }}

  # Chaos engineering team
  - name: 'chaos-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#chaos-engineering'
        title: 'üêí Chaos Engineering Alert'
        text: |
          *Chaos Event:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Experiment:* {{ .CommonLabels.experiment_name }}
          *Target:* {{ .CommonLabels.target_service }}
          *Status:* {{ .CommonLabels.chaos_status }}
          
          {{ range .Alerts }}
          *Details:* {{ .Annotations.description }}
          {{ end }}
        send_resolved: true

  # Warning alerts
  - name: 'warning-alerts'
    email_configs:
      - to: 'monitoring@nexus-v3.local'
        subject: '[WARNING] {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        body: |
          Warning Alert:
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Severity: {{ .Labels.severity }}
          Time: {{ .StartsAt }}
          {{ end }}

# Inhibition rules to prevent alert spam
inhibit_rules:
  # Inhibit warning alerts when critical alerts are firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service', 'instance']

  # Inhibit individual service alerts when entire cluster is down
  - source_match:
      alertname: 'ClusterDown'
    target_match_re:
      alertname: '(ServiceDown|HighErrorRate|HighLatency)'
    equal: ['cluster']

  # Inhibit SLO alerts when service is completely down
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      type: 'slo_violation'
    equal: ['service']

  # Inhibit performance budget alerts during chaos experiments
  - source_match:
      source: 'chaos_engineering'
    target_match:
      type: 'performance_budget'
    equal: ['service']

# Silence configuration
silences:
  # Maintenance windows
  - matchers:
      - name: maintenance
        value: "true"
    startsAt: "2024-01-01T00:00:00Z"
    endsAt: "2024-01-01T04:00:00Z"
    createdBy: "maintenance-system"
    comment: "Scheduled maintenance window"
